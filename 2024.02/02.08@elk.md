## ELK란?
- https://hudi.blog/elk-stack-overview/

## ELK 설치 및 filebeat로 nginx access log 수집하기
- https://www.elastic.co/kr/downloads
	- 무료 개방형 Elastic Stack 선택
- https://phoenixnap.com/kb/how-to-install-elk-stack-on-ubuntu

```shell
sudo -i; swapoff -a; exit
```
### Install Java, Nginx
```shell
# jdk 설치
sudo apt-get install openjdk-8-jdk
java -version
# nginx 설치
sudo apt-get install nginx
```

### Add Elastic Repository
```shell
# Elasticsearch용 PGP키 가져오기
wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
sudo apt-get install apt-transport-https
echo "deb https://artifacts.elastic.co/packages/7.x/apt stable main" | sudo tee –a /etc/apt/sources.list.d/elastic-7.x.list
```
- 7 버전으로 설치

### Install Elasticsearch
```shell
sudo apt-get update
sudo apt-get install elasticsearch
# 1
sudo vim /etc/elasticsearch/elasticsearch.yml

#network.host: 192.168.0.1
#http.port: 9200
=>
network.host: localhost # 수정
http.port: 9200
discovery.type: single-node # 추가

# 2
sudo vim /etc/elasticsearch/jvm.options

# 추가
-Xms512m
-Xmx512m

sudo systemctl start elasticsearch.service
sudo systemctl enable elasticsearch.service
curl -X GET "localhost:9200" --user elastic:3L5xGK9WNSiNVTPT5AHR

{
  "name" : "test",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "PrTln484Rli05y8HP2L7Vg",
  "version" : {
    "number" : "7.17.18",
    "build_flavor" : "default",
    "build_type" : "deb",
    "build_hash" : "8682172c2130b9a411b1bd5ff37c9792367de6b0",
    "build_date" : "2024-02-02T12:04:59.691750271Z",
    "build_snapshot" : false,
    "lucene_version" : "8.11.1",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "You Know, for Search"
}
```

### Install Kibana

```shell
sudo apt-get install kibana
sudo vim /etc/kibana/kibana.yml

#server.port: 5601
#server.host: "your-hostname"
#elasticsearch.hosts: ["http://localhost:9200"]
=>
server.port: 5601
server.host: "localhost"
elasticsearch.hosts: ["http://localhost:9200"]

sudo systemctl start kibana
sudo systemctl enable kibana

# 브라우저에서 실행
http://192.168.64.19:5601/

curl -XGET http://localhost:9200/_cat/indices?v
```

### Install Filebeat
```shell
sudo apt-get install filebeat
```

```yaml
# /etc/filebeat/filebeat.yml

...
# ============================== Filebeat inputs ===============================

filebeat.inputs:

# Each - is an input. Most options can be set at the input level, so
# you can use different inputs for various configurations.
# Below are the input specific configurations.

# filestream is an input for collecting log messages from files.
- type: log

  # Unique ID among all inputs, an ID is required.
  id: nginx-log

  # Change to true to enable this input configuration.
  enabled: true

  # Paths that should be crawled and fetched. Glob based paths.
  paths:
    - /var/log/nginx/access.log
    #- c:\programdata\elasticsearch\logs\*
    #document_type: syslog
...
# ================================== Outputs ===================================

# Configure what output to use when sending the data collected by the beat.

# ---------------------------- Elasticsearch Output ----------------------------
# logstash를 거쳐서 갈 것이기 때문에 주석 해제
#output.elasticsearch:
  # Array of hosts to connect to.
  #hosts: ["localhost:9200"]

  # Protocol - either `http` (default) or `https`.
  #protocol: "https"

  # Authentication credentials - either API key or username/password.
  #api_key: "id:api_key"
  #username: "elastic"
  #password: "changeme"

# ------------------------------ Logstash Output -------------------------------
output.logstash:
  # The Logstash hosts
  enabled : true
  hosts: ["localhost:5044"]

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  #ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  #ssl.key: "/etc/pki/client/cert.key"
...
```
- input 설정
- output을 elasticsearch => logstash로 설정
- elasticsearch 관련 주석 및 logstash 관련 주석 해제

```shell
cd modules.d/
mv system.yml system.yml.disabled
```
- 기본적으로  `Configured paths: [/var/log/auth.log* /var/log/secure*]` 등 system 관련 파일 또한 관련 로그를 파싱하는데, 이를 해제하면 더 보기 편함

```shell
sudo systemctl start filebeat
sudo systemctl enable filebeat
sudo systemctl restart filebeat
```
- https://velog.io/@tjrwn867/FileBeat

### Install Logstash
```shell
sudo apt-get install logstash
```

```config
# /etc/logstash/logstash.conf

# Sample Logstash configuration for creating a simple
# Beats -> Logstash -> Elasticsearch pipeline.

input {
  beats {
    port => 5044
  }
}

filter {
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
}


output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
    #index => "logs"
    user => "elastic"
    password => "3L5xGK9WNSiNVTPT5AHR"
  }
}
```
- filter를 통해 원하는 데이터 구조로 파싱
- output
	- hosts : elasticsearch 로 전송
	- index : 저장할 인덱스 설정(새로 생성도 됨)
	- user, password : https://principle486.tistory.com/entry/Attempted-to-resurrect-connection-to-dead-ES-instance-but-got-an-error

```yaml
# /etc/logstash/logstash.yaml

...
# ------------ Pipeline Configuration Settings --------------
#
# Where to fetch the pipeline configuration for the main pipeline
#
path.config: "/etc/logstash/logstash.conf"
#
...
```
- config file path 포함

```shell
sudo systemctl start logstash
sudo systemctl enable logstash
sudo systemctl status logstash

ps -ef | grep logstash
kill -9 30898
systemctl restart logstash.service
```

## metricbeat로 메트릭 정보 수집하기
```shell
sudo apt-get install metricbeat
sudo systemctl enable metricbeat
sudo systemctl start metricbeat
```

```yaml
# metricbeat.yml

metricbeat.modules:
- module: system
  metricsets:
    - cpu
    - memory
      #- network
      #- filesystem
      #- diskio
      #- process
  enabled: true
  period: 30s  # 30초마다 메트릭을 수집하도록 설정

output.logstash:
  # The Logstash hosts
  hosts: ["localhost:5044"]
```

```shell
sudo systemctl restart metricbeat
```

## k8s에 적용해보기
- elasticsearch 7.17.18 이미지로 컨테이너를 띄우면 지속적으로 에러 발생하면서 컨테이너가 꺼짐
- 가용 메모리의 문제
- https://stackoverflow.com/questions/77314599/elasticsearch-7-crashing-with-127-error-why
- 8 버전 이미지로 실행하니 잘 됨

### elasticsearch

```yaml
# elasticsearch-deploy.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: elasticsearch-config
  labels:
    app: elasticsearch
data:
  elasticsearch.yml: |-
    network.host: 0.0.0.0
    http.port: 9200
    discovery.type: single-node
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: elasticsearch-deployment
  labels:
    app: elasticsearch
spec:
  replicas: 1
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: elasticsearch:7.17.18
        env:
        - name: discovery.type
          value: "single-node"
        ports:
        - containerPort: 9200
        volumeMounts:
        - name: config-volume
          mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
          subPath: elasticsearch.yml
      volumes:
      - name: config-volume
        configMap:
          name: elasticsearch-config  # ConfigMap 이름을 여기에 입력합니다.
---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch-service
spec:
  selector:
    app: elasticsearch
  ports:
    - protocol: TCP
      name: kibana-port
      port: 9200
      targetPort: 9200
  type: LoadBalancer
```

```shell
k create -f elasticsearch-deploy.yaml

k delete cm elasticsearch-config
k delete deploy elasticsearch-deployment
k delete svc elasticsearch-service
```
- https://luvstudy.tistory.com/197
	- elasticsearch, kibana 설치

### kibana

```yaml
# kibana-deploy.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kibana-config
  labels:
    app: kibana
data:
  kibana.yml: |-
    server.port: 5601
    server.host: "0.0.0.0"
    elasticsearch.hosts: ["http://${ELASTICSEARCH_SERVICE_SERVICE_HOST}:9200"]
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana-deployment
  labels:
    app: kibana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      containers:
      - name: kibana
        image: kibana:7.17.18
        ports:
        - containerPort: 5601
        volumeMounts:
        - name: config-volume
          mountPath: /usr/share/kibana/config/kibana.yml
          subPath: kibana.yml
      volumes:
      - name: config-volume
        configMap:
          name: kibana-config  # ConfigMap 이름을 여기에 입력합니다.
---
apiVersion: v1
kind: Service
metadata:
  name: kibana-service
spec:
  selector:
    app: kibana
  ports:
    - protocol: TCP
      name: kibana-port
      port: 5601
      targetPort: 5601
  type: LoadBalancer
```

```shell
k create -f kibana-deploy.yaml

k delete cm kibana-config
k delete deploy kibana-deployment
k delete svc kibana-service

curl -XGET http://localhost:9200/_cat/indices?v
```

### metricbeat

```yaml
# metricbeat-kubernetes.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: metricbeat-daemonset-config
  namespace: kube-system
  labels:
    k8s-app: metricbeat
data:
  metricbeat.yml: |-
    metricbeat.config.modules:
      # Mounted `metricbeat-daemonset-modules` configmap:
      path: ${path.config}/modules.d/*.yml
      # Reload module configs as they change:
      reload.enabled: false

    processors:
      - add_cloud_metadata:

    cloud.id: ${ELASTIC_CLOUD_ID}
    cloud.auth: ${ELASTIC_CLOUD_AUTH}

    output.elasticsearch:
      hosts: ["http://${ELASTICSEARCH_SERVICE_SERVICE_HOST}:9200"]
      #protocol: "https"
      username: chanhyle
      password: chanhyle
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: metricbeat-daemonset-modules
  namespace: kube-system
  labels:
    k8s-app: metricbeat
data:
  system.yml: |-
    - module: system
      period: 10s
      metricsets:
        #- cpu
        #- load
        #- memory
        #- network
        - process
        #- process_summary
        #- core
        #- diskio
        #- socket
      processes: ['nginx']
      #process.include_top_n:
        #by_cpu: 5      # include top 5 processes by CPU
        #by_memory: 5   # include top 5 processes by memory
---
# Deploy a Metricbeat instance per node for node metrics retrieval
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: metricbeat
  namespace: kube-system
  labels:
    k8s-app: metricbeat
spec:
  selector:
    matchLabels:
      k8s-app: metricbeat
  template:
    metadata:
      labels:
        k8s-app: metricbeat
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      serviceAccountName: metricbeat
      terminationGracePeriodSeconds: 30
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
      - name: metricbeat
        image: docker.elastic.co/beats/metricbeat:7.17.18
        args: [
                #"-c", "/etc/metricbeat.yml",
          "-e",
          "-system.hostfs=/hostfs",
        ]
        env:
        - name: ELASTICSEARCH_HOST
          value: elasticsearch
        - name: ELASTICSEARCH_PORT
          value: "9200"
        - name: ELASTICSEARCH_USERNAME
          value: elastic
        - name: ELASTICSEARCH_PASSWORD
          value: changeme
        - name: ELASTIC_CLOUD_ID
          value:
        - name: ELASTIC_CLOUD_AUTH
          value:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        securityContext:
          runAsUser: 0
          # If using Red Hat OpenShift uncomment this:
          #privileged: true
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: config
          mountPath: /usr/share/metricbeat/metricbeat.yml
          readOnly: true
          subPath: metricbeat.yml
        - name: data
          mountPath: /usr/share/metricbeat/data
        - name: modules
          mountPath: /usr/share/metricbeat/modules.d
          readOnly: true
        - name: proc
          mountPath: /hostfs/proc
          readOnly: true
        - name: cgroup
          mountPath: /hostfs/sys/fs/cgroup
          readOnly: true
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: cgroup
        hostPath:
          path: /sys/fs/cgroup
      - name: config
        configMap:
          defaultMode: 0640
          name: metricbeat-daemonset-config
      - name: modules
        configMap:
          defaultMode: 0640
          name: metricbeat-daemonset-modules
      - name: data
        hostPath:
          # When metricbeat runs as non-root user, this directory needs to be writable by group (g+w)
          path: /var/lib/metricbeat-data
          type: DirectoryOrCreate
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: metricbeat
subjects:
- kind: ServiceAccount
  name: metricbeat
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: metricbeat
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: metricbeat
  namespace: kube-system
subjects:
  - kind: ServiceAccount
    name: metricbeat
    namespace: kube-system
roleRef:
  kind: Role
  name: metricbeat
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: metricbeat-kubeadm-config
  namespace: kube-system
subjects:
  - kind: ServiceAccount
    name: metricbeat
    namespace: kube-system
roleRef:
  kind: Role
  name: metricbeat-kubeadm-config
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: metricbeat
  labels:
    k8s-app: metricbeat
rules:
- apiGroups: [""]
  resources:
  - nodes
  - namespaces
  - events
  - pods
  - services
  verbs: ["get", "list", "watch"]
# Enable this rule only if planing to use Kubernetes keystore
#- apiGroups: [""]
#  resources:
#  - secrets
#  verbs: ["get"]
- apiGroups: ["extensions"]
  resources:
  - replicasets
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources:
  - statefulsets
  - deployments
  - replicasets
  verbs: ["get", "list", "watch"]
- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["get", "list", "watch"]
- apiGroups:
  - ""
  resources:
  - nodes/stats
  verbs:
  - get
- nonResourceURLs:
  - "/metrics"
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: metricbeat
  # should be the namespace where metricbeat is running
  namespace: kube-system
  labels:
    k8s-app: metricbeat
rules:
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs: ["get", "create", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: metricbeat-kubeadm-config
  namespace: kube-system
  labels:
    k8s-app: metricbeat
rules:
  - apiGroups: [""]
    resources:
      - configmaps
    resourceNames:
      - kubeadm-config
    verbs: ["get"]
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: metricbeat
  namespace: kube-system
  labels:
    k8s-app: metricbeat
```

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: metricbeat-daemonset-modules
  namespace: kube-system
  labels:
    k8s-app: metricbeat
data:
  system.yml: |-
    - module: nginx
      period: 10s
      metricsets: ["stubstatus"]
      enabled: true
      hosts: ["http://localhost:31139"]
      server_status_path: "nginx_status"
```

```yaml
# /etc/nginx/conf.d/deafult.conf
...

# 추가
location = /nginx_status {
    stub_status;
}
...
```

```shell
nginx -s reload
```
- nginx 모듈
	- hosts가 내부 IP:port를 사용하지 못하는가?
	- 10.97.189.29:80


```shell
k delete cm metricbeat-daemonset-config -n kube-system
k delete cm metricbeat-daemonset-modules -n kube-system
k delete ds metricbeat -n kube-system
k delete clusterRoleBinding metricbeat
k delete roleBinding metricbeat -n kube-system
k delete roleBinding metricbeat-kubeadm-config -n kube-system
k delete clusterRole metricbeat
k delete role metricbeat -n kube-system
k delete role metricbeat-kubeadm-config -n kube-system
k delete serviceAccount metricbeat -n kube-system

k create -f metricbeat-kubernetes.yaml

k get pod -n kube-system
```

- https://www.elastic.co/guide/en/beats/metricbeat/current/running-on-kubernetes.html

```shell
siege -c10 -d0.5 http://192.168.64.17:31139/
```

### 고려할 것
- namespace를 어떻게 관리할 것인가?
	- 서비스 별로?
- kibana dashboard 미리 설정된 템플릿을 자동으로 적용시킬 수 있는가?
- security
- v7? ~~v8?~~
- logstash pod를 올리면 부하가 심해짐 => 쓰지 않을 것인가?
- metricbeat가 어떤 서버의 정보를 가지고 오는 것인가?
	- 현재 세팅은 어디 범위까지 정보를 수집하는 것인가?
	- 특정 pod의 메트릭 정보를 가져올 수 있는가? 그렇다면 어떻게 세팅하는가?
- kibana를 커스터마이징 할 수 있는가?
- EKS 전용 elk를 사용할 수 있는가?
	- 먼저 알아보기
## 더 알아보기

### security

```
systemctl start elasticsearch.service

add elasticsearch.passwordelasticsearch-setup-passwords auto / interactive

Changed password for user apm_system
PASSWORD apm_system = VYY1gGhGZXN2OY0lqgTW

Changed password for user kibana_system
PASSWORD kibana_system = XNtSWwMger4nrExsDytz

Changed password for user kibana
PASSWORD kibana = XNtSWwMger4nrExsDytz

Changed password for user logstash_system
PASSWORD logstash_system = OncXvl364FutOIKkdX6D

Changed password for user beats_system
PASSWORD beats_system = xCeByAeyRTomHLtkOLCh

Changed password for user remote_monitoring_user
PASSWORD remote_monitoring_user = 8c9qJb9bbcn4Fz8DUvkZ

Changed password for user 
PASSWORD elastic = 3L5xGK9WNSiNVTPT5AHR
```
- https://www.elastic.co/guide/en/elasticsearch/reference/7.16/security-minimal-setup.html
- https://www.elastic.co/guide/en/elasticsearch/reference/current/configuring-stack-security.html
- kibana에 로그인할 때는 `elastic`/passwd으로 로그인
- `kibana_system`는 kibana에 권한이 없기 때문

```shell
# v8
# 토큰 발급 과정 필요
k exec -it elasticsearch-deployment-6c9547c57b-j5kgp -- bash
./bin/elasticsearch-create-enrollment-token --scope kibana

WARNING: Owner of file [/usr/share/elasticsearch/config/users] used to be [root], but now is [elasticsearch]
WARNING: Owner of file [/usr/share/elasticsearch/config/users_roles] used to be [root], but now is [elasticsearch]
eyJ2ZXIiOiI4LjQuMyIsImFkciI6WyIxMC4wLjEuNDE6OTIwMCJdLCJmZ3IiOiIwOTU4MjI3ZWI1YjFkN2U1YjE2YWNkN2Y4NmNlYjg4MmM2MDVkZjRkNjhhNWEzZGNlNzE2ZjZjOWJlYjhiOTQxIiwia2V5IjoiSG1KVG9ZMEJxcmRTWllZSEN6Tkk6bHRHQi1kZTBUZkdVODRaVnQydFM4USJ9

# 토큰을 http://192.168.64.17:32561/ 에 입력
# verification code 입력 필요

k exec -it kibana-deployment-57cf4db67f-gtjwf -- bash
./bin/kibana-verification-code
Your verification code is:  712 015

# 유저 생성
k exec -it elasticsearch-deployment-6c9547c57b-j5kgp -- bash
./bin/elasticsearch-users useradd chanhyle -p chanhyle -r superuser
```
### core dns
- https://jonnung.dev/kubernetes/2020/05/11/kubernetes-dns-about-coredns/
### eck
- https://hmj2088.medium.com/elastic-cloud-on-kubernetes-b5a69339e920

 : nginx
10.100.53.240 : elasticsearch
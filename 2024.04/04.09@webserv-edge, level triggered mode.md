## Edge-Triggered, Level-Triggered

- `kqueue`와 `epoll`과 같은 이벤트 기반 I/O Multiplexing 함수에서 Edge-Triggered(ET), Level-Triggered(LT) 모드를 선택하여 사용할 수 있다
- 둘 사이에는 입출력 이벤트를 처리하는데 방식의 차이가 존재한다
- 이벤트 발생?
	- 소켓의 상태 변화를 감지하는 것
	- 예를 들어, 소켓에서 데이터를 읽거나 쓰는 작업이 완료되었을 때, 소켓이 연결되었거나 연결이 종료되었을 때 등 다양한 상황에서 이벤트가 발생할 수 있습니다.
	- recv 함수를 호출하여 데이터를 읽어오는 것은 소켓의 상태 변화에 직접적인 영향을 주지 않나?
- ET
	- 한 번의 이벤트 발생 후에는 해당 소켓의 상태가 변하지 않는 한 다시 이벤트가 발생하지 않습니다
	- 즉, 이벤트가 발생한 후 처리되지 않은 상태에서 같은 이벤트가 다시 발생해도 보고되지 않음
	- 새로운 이벤트를 등록해야 함
- LT
	- 소켓 버퍼에 데이터가 존재하는 한 계속해서 이벤트가 발생합니다
	- 데이터를 읽을 수 있는 상태일 때까지 반복해서 이벤트를 처리합니다

## `client.cpp`

```cpp
#include <signal.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <sys/types.h>
#include <sys/socket.h>
#include <iostream>
#include <chrono>
#include <fcntl.h>
# include <semaphore.h>

#define BUFSIZE 30001
#define PROC_COUNT 100

char BUF[BUFSIZE];
int pids[PROC_COUNT];

sem_t		*sem_end1;

void error_handling(const char *message)
{
	std::cerr << message << std::endl;
	exit(1);
}

int main(int argc, char **argv)
{
	struct sockaddr_in serv_addr;
	char message[BUFSIZE];
	int str_len  = 100;
	
	sem_unlink("sem_end1");
	sem_end1 = sem_open("sem_end1", O_CREAT, 0600, 1);
	if (sem_end1 == SEM_FAILED)
		error_handling("SEM_FAILED");
	if(argc != 2)
	{
		printf("Usage : %s <IP> <port> \n", argv[0]);
		exit(1);
	}
	memset(&serv_addr, 0, sizeof(serv_addr));
	serv_addr.sin_family = AF_INET;
	serv_addr.sin_addr.s_addr = htonl(2130706433); //127.0.0.1
	serv_addr.sin_port = htons(atoi(argv[1]));
	auto start_total = std::chrono::high_resolution_clock::now();
	for (int i = 0; i < PROC_COUNT; i++) {
		usleep(100);
		int pid = fork();
		if (pid == 0) {
			int sock = socket(PF_INET, SOCK_STREAM, 0);
			// std::cout << "1\n";
			if(sock == -1)
				error_handling("socket() error");	
			// std::cout << "2\n";
			if(connect(sock, (struct sockaddr *)&serv_addr, sizeof(serv_addr)) == -1)
				error_handling("connect() error");
			// std::cout << "3\n";
			// fcntl(sock, F_SETFL, O_NONBLOCK);
			int fd = open("test2.txt", O_RDONLY);
			read(fd, BUF, 30000);
			// std::cout << "4\n";
			// start
			auto start = std::chrono::high_resolution_clock::now();
			/* 소켓 연결 */
			send(sock, BUF, strlen(BUF), 0);
			/* 메시지 수신 출력 */
			memset(BUF, 0, 30000);
			str_len = read(sock, message, BUFSIZE);
			// std::cout << "5\n";
			message[str_len] = 0;
			// printf("서버로부터 전송된 메시지 : %s\n", message);
			// end
			// 시간 측정 결과 계산
			auto end = std::chrono::high_resolution_clock::now();
			auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
			sem_wait(sem_end1);
			// std::cout << "작업 수행 시간: " << duration.count() << " microseconds\n";
			std::cout << duration.count() << "\n";
			sem_post(sem_end1);	
			exit(0);
		} else {
			pids[i] = pid;
		}
	}
	int status;
    for (int i = 0; i < PROC_COUNT; ++i) {
        waitpid(pids[i], &status, 0);
    }
	// 시간 측정 결과 계산
	auto end_total = std::chrono::high_resolution_clock::now();
	auto duration_total = std::chrono::duration_cast<std::chrono::microseconds>(end_total - start_total);
	std::cerr << "전체 작업 수행 시간: " << duration_total.count() << " microseconds\n";
	return 0;
}
```
- `fork` 를 통해 여러 클라이언트를 서버에 연결
- `usleep` 으로 side effect 처리
- 프로세스 개수(`fork` count) 지정 필요

## `Edge-Triggered.cpp`

```cpp
#include <vector>
#include <errno.h>
#include <err.h>
#include <string.h>
#include <unistd.h>
#include <netdb.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <sys/event.h>
#include <stdlib.h>
#include <stdio.h>
#include <fcntl.h>
#include <iostream>

const int BUF_SIZE = 42 * 4096;

typedef struct s_client {
	int	id;
    char message[BUF_SIZE];
}   t_client;

t_client    client[1024];
fd_set readfds, allfds, writefds;
int id = 0, maxfd = 0;
char sendBuff[10];
char recvBuff[BUF_SIZE];

void	print_error(const char * msg)
{
	write(STDERR_FILENO, msg, strlen(msg));
	exit(EXIT_FAILURE);
}

int main(int argc, char *argv[])
{
	int sockfd, connfd, len, fd;
	struct sockaddr_in servaddr, cli; 

	if (argc != 2)
		print_error("Wrong number of arguments\n");
    memset(client, 0, sizeof(client));
	sockfd = socket(AF_INET, SOCK_STREAM, 0); 
	if (sockfd == -1)
		print_error("Fatal error\n");
	int reuse = 1;
    setsockopt(sockfd, SOL_SOCKET, SO_REUSEADDR, &reuse, sizeof(reuse));
    memset(&servaddr, 0, sizeof(servaddr));
	bzero(&servaddr, sizeof(servaddr)); 
	memset(recvBuff, 0, strlen(recvBuff));
	// assign IP, PORT 
	servaddr.sin_family = AF_INET; 
	servaddr.sin_addr.s_addr = htonl(2130706433); //127.0.0.1
	servaddr.sin_port = htons(atoi(argv[1])); 
  
	// Binding newly created socket to given IP and verification 
	if ((bind(sockfd, (const struct sockaddr *)&servaddr, sizeof(servaddr))) != 0)
		print_error("Fatal error\n"); 
	if (listen(sockfd, 128) != 0)
		print_error("Fatal error\n");

	std::vector<struct kevent> event; /* Event we want to monitor */
  	struct kevent tevent[10]; /* Event triggered */
  	int kq, ret;
	struct kevent temp;

	kq = kqueue();
	EV_SET(&temp, sockfd, EVFILT_READ, EV_ADD | EV_CLEAR | EV_ENABLE, 0, 0, NULL);
	event.push_back(temp);
	while (1) 
	{	
		int   res = kevent(kq, &event[0], event.size(), tevent, 10, NULL);
		event.clear();
		std::cout << "res " << res << std::endl;
		if (res == -1)
            continue;
		for (int i = 0; i < res; i++) {
			if (tevent[i].filter == EVFILT_READ) {
				if (tevent[i].ident == sockfd) {
					len = sizeof(cli);
					connfd = accept(sockfd, (struct sockaddr *)&cli, (socklen_t*)&len);
					if (connfd < 0)
                        continue;
					else
                    {
                        // 네트워크 문제 등에 의해 블로킹 될 수 있으므로 논블로킹 모드 설정
                        client[connfd].id = id;
                        id++;
						struct kevent clientevent;
						// 한 번의 이벤트에 EVFILT_READ와 EVFILT_WRITE 이벤트가 동시에 처리될 수는 없음
						EV_SET(&clientevent, connfd, EVFILT_READ, EV_ONESHOT, 0, 0, NULL);
						event.push_back(clientevent);
                        // printf("server: client just arrived\n");
                    }	
				}
				else {
					char tmp[256];
					memset(tmp, 0, 256);
					int n = 256;
					while (n == 256) {
						n = recv(tevent[i].ident, tmp, 256, 0);
						strncat(client[tevent[i].ident].message, tmp, strlen(tmp));
						// std::cout << strlen(client[tevent[i].ident].message) << std::endl;
						memset(tmp, 0, 256);
						if (strlen(client[tevent[i].ident].message) >= 29900) {
							struct kevent clientevent;
							memset(client[tevent[i].ident].message, 0, strlen(client[tevent[i].ident].message));
							EV_SET(&clientevent, tevent[i].ident, EVFILT_WRITE, EV_ADD, 0, 0, NULL);
							event.push_back(clientevent);
						}
					}
				}
			}
			if (tevent[i].filter == EVFILT_WRITE) {
				memset(sendBuff, 0, 10);
				strncat(sendBuff, "done", strlen("done"));
				send(tevent[i].ident, sendBuff, strlen(sendBuff), 0);
				memset(sendBuff, 0, 10);
				struct kevent clientevent;
				EV_SET(&clientevent, tevent[i].ident, EVFILT_WRITE, EV_DELETE, 0, 0, NULL);
				event.push_back(clientevent);
			}
		}
	}
}
```

## `Level-Triggered.cpp`

```cpp
#include <vector>
#include <errno.h>
#include <err.h>
#include <string.h>
#include <unistd.h>
#include <netdb.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <sys/event.h>
#include <stdlib.h>
#include <stdio.h>
#include <fcntl.h>
#include <iostream>

const int BUF_SIZE = 42 * 4096;

typedef struct s_client {
	int	id;
    char message[BUF_SIZE];
}   t_client;

t_client    client[1024];
fd_set readfds, allfds, writefds;
int id = 0, maxfd = 0;
char sendBuff[10];
char recvBuff[BUF_SIZE];

void	print_error(const char * msg)
{
	write(STDERR_FILENO, msg, strlen(msg));
	exit(EXIT_FAILURE);
}

int main(int argc, char *argv[])
{
	int sockfd, connfd, len, fd;
	struct sockaddr_in servaddr, cli; 

	if (argc != 2)
		print_error("Wrong number of arguments\n");
    memset(client, 0, sizeof(client));
	sockfd = socket(AF_INET, SOCK_STREAM, 0); 
	if (sockfd == -1)
		print_error("Fatal error\n");
	int reuse = 1;
    setsockopt(sockfd, SOL_SOCKET, SO_REUSEADDR, &reuse, sizeof(reuse));
    memset(&servaddr, 0, sizeof(servaddr));
	bzero(&servaddr, sizeof(servaddr)); 
	memset(recvBuff, 0, strlen(recvBuff));
	// assign IP, PORT 
	servaddr.sin_family = AF_INET; 
	servaddr.sin_addr.s_addr = htonl(2130706433); //127.0.0.1
	servaddr.sin_port = htons(atoi(argv[1])); 
  
	// Binding newly created socket to given IP and verification 
	if ((bind(sockfd, (const struct sockaddr *)&servaddr, sizeof(servaddr))) != 0)
		print_error("Fatal error\n"); 
	if (listen(sockfd, 128) != 0)
		print_error("Fatal error\n");

	std::vector<struct kevent> event; /* Event we want to monitor */
  	struct kevent tevent[10]; /* Event triggered */
  	int kq, ret;
	struct kevent temp;

	kq = kqueue();
	EV_SET(&temp, sockfd, EVFILT_READ, EV_ADD | EV_CLEAR | EV_ENABLE, 0, 0, NULL);
	event.push_back(temp);
	while (1) 
	{	
		int   res = kevent(kq, &event[0], event.size(), tevent, 10, NULL);
		event.clear();
		std::cout << "res " << res << std::endl;
		if (res == -1)
            continue;
		for (int i = 0; i < res; i++) {
			if (tevent[i].filter == EVFILT_READ) {
				if (tevent[i].ident == sockfd) {
					len = sizeof(cli);
					connfd = accept(sockfd, (struct sockaddr *)&cli, (socklen_t*)&len);
					if (connfd < 0)
                        continue;
					else
                    {
                        // 네트워크 문제 등에 의해 블로킹 될 수 있으므로 논블로킹 모드 설정
                        client[connfd].id = id;
                        id++;
						struct kevent clientevent;
						EV_SET(&clientevent, connfd, EVFILT_READ, EV_ADD, 0, 0, NULL);
						event.push_back(clientevent);
                        // printf("server: client just arrived\n");
                    }	
				}
				else {
					char tmp[256];
					memset(tmp, 0, 256);
					int n = recv(tevent[i].ident, tmp, 256, 0);
					strncat(client[tevent[i].ident].message, tmp, strlen(tmp));
					memset(tmp, 0, 256);
					// std::cout << strlen(client[tevent[i].ident].message) << std::endl;
					if (strlen(client[tevent[i].ident].message) >= 29900) {
						struct kevent clientevent;
						memset(client[tevent[i].ident].message, 0, strlen(client[tevent[i].ident].message));
						EV_SET(&clientevent, tevent[i].ident, EVFILT_WRITE, EV_ADD, 0, 0, NULL);
						event.push_back(clientevent);
					}
				}
			}
			if (tevent[i].filter == EVFILT_WRITE) {
				memset(sendBuff, 0, 10);
				strncat(sendBuff, "done", strlen("done"));
				send(tevent[i].ident, sendBuff, strlen(sendBuff), 0);
				memset(sendBuff, 0, 10);
				struct kevent clientevent;
				struct kevent clientevent2;
				// 한 번의 이벤트에 EVFILT_READ와 EVFILT_WRITE 이벤트가 동시에 처리될 수는 없음
				EV_SET(&clientevent, tevent[i].ident, EVFILT_WRITE, EV_DELETE, 0, 0, NULL);
				EV_SET(&clientevent2, tevent[i].ident, EVFILT_READ, EV_DELETE, 0, 0, NULL);
				event.push_back(clientevent);
				event.push_back(clientevent2);
			}
		}
	}
}
```

## 결과

### 1. 

- 조건
	- 클라이언트 개수 : 10
	- `usleep` : 100
	- `send` 길이 : 30,000
- ET
```shell
# 전체 작업 수행 시간: 3444 microseconds

cat edge_kevent_count.txt | wc -l
#     7
node avg_returned_time.js
# edge.txt 334.2 10
```

- LT
```shell
# 전체 작업 수행 시간: 3698 microseconds

cat level_kevent_count.txt | wc -l
#       120
node avg_returned_time.js
# level.txt 524.2 10
```
- 메시지 크기가 클 경우, ET보다 LT에서 `kevent` 함수가 더 많이 호출된다
	- 평균 반환 시간이 약 2.6배, 전체 작업 수행 시간은 약 1.7배 더 많이 소요된다
	- `kevent` 시스템 콜 호출 횟수가 증가함에 따라 수행 시간이 더 늘어나는 것을 확인할 수 있다

### 2.

- 조건
	- 클라이언트 개수 : 100
	- `usleep` : 100
	- `send` 길이 : 30,000
- ET
```shell
# 전체 작업 수행 시간: 28729 microseconds

cat edge_kevent_count.txt | wc -l
#     217
node avg_returned_time.js
# edge.txt 305.01 100
```

- LT
```shell
# 전체 작업 수행 시간: 32009 microseconds

cat level_kevent_count.txt | wc -l
#       1372
node avg_returned_time.js
# level.txt 543.24 100
```
- 보내는 메시지의 길이가 길면
	- ET는 클라이언트 개수에 비례해도 반환 시간이 일정한 반면
	- LT는 편차가 크다
- 전체 작업 수행 시간이 LT가 길다

### 3.

- 조건
	- 클라이언트 개수 : 500
	- `usleep` : 100
	- `send` 길이 : 30,000
- ET
```shell
# 전체 작업 수행 시간: 152858 microseconds

cat edge_kevent_count.txt | wc -l
#     1336
node avg_returned_time.js
# edge.txt 306.764 500
```

- LT
```shell
# 전체 작업 수행 시간: 154463 microseconds

cat level_kevent_count.txt | wc -l
#       6147
node avg_returned_time.js
# level.txt 590.404 500
```
- `kevent` 호출 수에 비례하여 전체 작업 수행 시간이 LT가 길다

### 4.

- 조건
	- 클라이언트 개수 : 10
	- `usleep` : 100
	- `send` 길이 : 100
- ET
```shell
# 전체 작업 수행 시간: 3239 microseconds

node avg_returned_time.js
# edge.txt 64.7 10
```

- LT
```shell
# 전체 작업 수행 시간: 3201 microseconds

node avg_returned_time.js
# level.txt 74.7 10
```
- 한 번의 `read`로 두 경우 모두 I/O가 완료되기 때문에, 큰 차이는 없다

### 5.

- 조건
	- 클라이언트 개수 : 100
	- `usleep` : 100
	- `send` 길이 : 100
- ET
```shell
# 전체 작업 수행 시간: 26293 microseconds

node avg_returned_time.js
# edge.txt 63.34 100
```

- LT
```shell
# 전체 작업 수행 시간: 28857 microseconds

node avg_returned_time.js
# level.txt 67.68 100
```
- 보내는 메시지의 길이 짧다면
	- ET, LT 모두 반환 시간 측면에서 큰 차이가 없다
- 내부적으로 I/O 로직이 비슷하기 때문에